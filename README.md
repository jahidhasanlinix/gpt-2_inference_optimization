# ğŸ§  GPT-2 Inference API with FastAPI on Google Colab

This project demonstrates how to deploy a Hugging Face GPT-2 model for inference using FastAPI, hosted directly from a Google Colab notebook. It also includes a separate client notebook to test both single and batch inference via HTTP.

---

## ğŸš€ 1. GPT-2 Inference Server (FastAPI + ngrok)

**Run this notebook to:**
- Load a GPT-2 model using Hugging Face Transformers
- Serve it using FastAPI
- Expose the API publicly using ngrok

ğŸ‘‰ [**Open Inference Server Notebook**](https://colab.research.google.com/drive/1fREdBR0K0spA-kTzKdtM4Y0pL6quQola?usp=sharing)

### ğŸ› ï¸ Features
- `/generate` endpoint for single-prompt inference
- `/batch_generate` endpoint for multi-prompt batch inference
- Automatic use of GPU (if available)
- Lightweight and ideal for prototyping & demos

---

## ğŸ§ª 2. API Client Notebook

**Use this notebook to:**
- Send test requests to your public FastAPI endpoint
- Measure tokens generated and latency
- Support both single and batch inference calls

ğŸ‘‰ [**Open Client Test Notebook**](https://colab.research.google.com/drive/1hrwb8K3Iu39WnI3yXZOVytAikQqjwPnS?usp=sharing)

---

## ğŸ“£ 3. Use Cases

âš¡ Fast prototyping with Hugging Face models

ğŸ”Œ Running inference APIs directly inside notebooks

ğŸ“ Teaching, tutorials, and quick ML demos

ğŸ§ª Experimenting with quantization, batching, and latency

---

## ğŸ™Œ Credits

Built with:

- âš¡ [FastAPI](https://fastapi.tiangolo.com/) â€“ High-performance Python web framework for building APIs  
- ğŸ¤— [Hugging Face Transformers](https://huggingface.co/transformers/) â€“ State-of-the-art NLP models  
- ğŸ§  [Google Colab](https://colab.research.google.com/) â€“ Free cloud notebooks with GPU support  
- ğŸŒ [ngrok](https://ngrok.com/) â€“ Public URLs for localhost APIs
