{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken API_KEY_HERE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY6PNbTUyhuX",
        "outputId": "553828c7-b2ce-4432-d288-1911d3eb0300"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ngrok: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TgjROoGwyYq7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_URL = \"https://e211-34-86-98-232.ngrok-free.app\""
      ],
      "metadata": {
        "id": "CC_XK7QlzEE3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test single inference\n",
        "def test_single_inference():\n",
        "    url = f\"{NGROK_URL}/generate\"\n",
        "    payload = {\n",
        "        \"prompt\": \"The future of artificial intelligence is\",\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "    start_time = time.time()\n",
        "    response = requests.post(url, json=payload)\n",
        "    end_time = time.time()\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        print(f\"Generated: {result['generated_text']}\")\n",
        "        print(f\"Tokens: {result['tokens_generated']}\")\n",
        "        print(f\"Server inference time: {result['inference_time']:.3f}s\")\n",
        "        print(f\"Total request time: {end_time - start_time:.3f}s\")\n",
        "    else:\n",
        "        print(f\"Error: {response.text}\")\n",
        "\n",
        "# Test batch inference\n",
        "def test_batch_inference():\n",
        "    url = f\"{NGROK_URL}/batch_generate\"\n",
        "    payload = {\n",
        "        \"prompts\": [\n",
        "            \"AI will revolutionize\",\n",
        "            \"Machine learning helps\",\n",
        "            \"The future holds\"\n",
        "        ],\n",
        "        \"max_tokens\": 30\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        print(f\"Batch size: {result['batch_size']}\")\n",
        "        print(f\"Total time: {result['total_inference_time']:.3f}s\")\n",
        "        print(f\"Avg time per prompt: {result['average_time_per_prompt']:.3f}s\")\n",
        "\n",
        "        for res in result['results']:\n",
        "            print(f\"Prompt {res['prompt_index']}: {res['generated_text']}\")\n",
        "    else:\n",
        "        print(f\"Error: {response.text}\")\n",
        "\n",
        "# Run both tests\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing single inference...\")\n",
        "    test_single_inference()\n",
        "\n",
        "    print(\"\\nTesting batch inference...\")\n",
        "    test_batch_inference()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcMgvMPqzHV-",
        "outputId": "f8d6646d-e1b1-4cc5-b8b2-2e35d30c9ee1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing single inference...\n",
            "Generated: The future of artificial intelligence is not what is being discussed.\n",
            "Tokens: 7\n",
            "Server inference time: 0.467s\n",
            "Total request time: 0.594s\n",
            "\n",
            "Testing batch inference...\n",
            "Batch size: 3\n",
            "Total time: 0.625s\n",
            "Avg time per prompt: 0.208s\n",
            "Prompt 0: AI will revolutionize video game graphics.\n",
            "Prompt 1: Machine learning helpsNot make mistakes.\n",
            "Prompt 2: The future holdsAnd holds up.\n"
          ]
        }
      ]
    }
  ]
}