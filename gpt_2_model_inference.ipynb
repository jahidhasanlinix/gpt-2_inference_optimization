{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8012267ae7dc43e18dcc248441077784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_895cabbe8d3e445aaf21232afa306fb5",
              "IPY_MODEL_5891a3e1dba0459aad1ba1c529d63601",
              "IPY_MODEL_edee89e07bd648798b98a8053bd7ca05"
            ],
            "layout": "IPY_MODEL_307c3f095d2f48ce8af17353410ed134"
          }
        },
        "895cabbe8d3e445aaf21232afa306fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b902fe333f0b4d57aa40d45d8a3beaf9",
            "placeholder": "​",
            "style": "IPY_MODEL_0dfffcfa5b69473fb958d3f78e968fde",
            "value": "config.json: 100%"
          }
        },
        "5891a3e1dba0459aad1ba1c529d63601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dfbc024a76a430c94b7b64ca35fd922",
            "max": 641,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98576cea912d4ff2a55f329bbb090746",
            "value": 641
          }
        },
        "edee89e07bd648798b98a8053bd7ca05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_271bb0cd79314011b7fc79869495a7f7",
            "placeholder": "​",
            "style": "IPY_MODEL_166e4b84f6854d889146474f910dbe86",
            "value": " 641/641 [00:00&lt;00:00, 21.4kB/s]"
          }
        },
        "307c3f095d2f48ce8af17353410ed134": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b902fe333f0b4d57aa40d45d8a3beaf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dfffcfa5b69473fb958d3f78e968fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dfbc024a76a430c94b7b64ca35fd922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98576cea912d4ff2a55f329bbb090746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "271bb0cd79314011b7fc79869495a7f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "166e4b84f6854d889146474f910dbe86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d33ac0aed9f14b83bde22a89a0a77975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e21a2f5dd5745db9c1887bdbb1b4ac3",
              "IPY_MODEL_772d80382ea645159e7799cf48bdaffa",
              "IPY_MODEL_030fc99e8b6f462491792db5033d7e74"
            ],
            "layout": "IPY_MODEL_4f3d06116d40496eb9f9e26d5459c096"
          }
        },
        "0e21a2f5dd5745db9c1887bdbb1b4ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_211bbeaae23c4e399f06749b51ad7190",
            "placeholder": "​",
            "style": "IPY_MODEL_dd6e459c45db4f5ea18600b808bb8333",
            "value": "model.safetensors: 100%"
          }
        },
        "772d80382ea645159e7799cf48bdaffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0b6f3b97a8a4dfe8e4036cc1064af28",
            "max": 351256598,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ac873fe7be44f84a3fd0349a93bdd9e",
            "value": 351256598
          }
        },
        "030fc99e8b6f462491792db5033d7e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4baf47ca23c040e78f0a935b6e9b2269",
            "placeholder": "​",
            "style": "IPY_MODEL_5d29d80261d24f919faa666d51b539c5",
            "value": " 351M/351M [00:01&lt;00:00, 239MB/s]"
          }
        },
        "4f3d06116d40496eb9f9e26d5459c096": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "211bbeaae23c4e399f06749b51ad7190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd6e459c45db4f5ea18600b808bb8333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0b6f3b97a8a4dfe8e4036cc1064af28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ac873fe7be44f84a3fd0349a93bdd9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4baf47ca23c040e78f0a935b6e9b2269": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d29d80261d24f919faa666d51b539c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c062447b0b6245c4ad73e736310ffd5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ccb81728fd0e43a08dda2118b610bbe0",
              "IPY_MODEL_ba0be84f70fa4f3ea87ecf919c23d365",
              "IPY_MODEL_8cc08cca64734142991ab0f874cae0b0"
            ],
            "layout": "IPY_MODEL_4da0c81e2911432ab3a9ed3b38cd3573"
          }
        },
        "ccb81728fd0e43a08dda2118b610bbe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f69276926354b0a8900716dfcb5cce4",
            "placeholder": "​",
            "style": "IPY_MODEL_859ca557dcb7432bbda46b86ce2f7fff",
            "value": "generation_config.json: 100%"
          }
        },
        "ba0be84f70fa4f3ea87ecf919c23d365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07f835d622644e6895c216c76d348b37",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5530599f7d2b49dfb2d149e2378726aa",
            "value": 124
          }
        },
        "8cc08cca64734142991ab0f874cae0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14f004052bc54a6c82c9c045decf18cf",
            "placeholder": "​",
            "style": "IPY_MODEL_dd26dea94be4479d9ca5404b1931fa8d",
            "value": " 124/124 [00:00&lt;00:00, 6.65kB/s]"
          }
        },
        "4da0c81e2911432ab3a9ed3b38cd3573": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f69276926354b0a8900716dfcb5cce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "859ca557dcb7432bbda46b86ce2f7fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07f835d622644e6895c216c76d348b37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5530599f7d2b49dfb2d149e2378726aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14f004052bc54a6c82c9c045decf18cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd26dea94be4479d9ca5404b1931fa8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1aA0vbDcppkD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4RRjTob9pJH4"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers torch accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers --upgrade"
      ],
      "metadata": {
        "id": "10rtLRhIqCOD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "Ew52djF_qmee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "CgegmwZtpvNx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load GPT-2 Model"
      ],
      "metadata": {
        "id": "6GZbHjz6qj4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "wz0rQaUyp38w"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXgxHagGtH_V",
        "outputId": "0e89bb34-b9d5-4e8a-ac51-999c98282891"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchinfo"
      ],
      "metadata": {
        "id": "e5fwv81UtOGT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model, input_size=(1, 10), dtypes=[torch.long])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ri_xS9QtLyO",
        "outputId": "bd70e5b9-01d6-4de6-a35d-281707b81669"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "GPT2LMHeadModel                                    [1, 12, 10, 64]           --\n",
              "├─GPT2Model: 1-1                                   [1, 12, 10, 64]           --\n",
              "│    └─Embedding: 2-1                              [1, 10, 768]              38,597,376\n",
              "│    └─Embedding: 2-2                              [1, 10, 768]              786,432\n",
              "│    └─Dropout: 2-3                                [1, 10, 768]              --\n",
              "│    └─ModuleList: 2-4                             --                        --\n",
              "│    │    └─GPT2Block: 3-1                         [1, 10, 768]              7,087,872\n",
              "│    │    └─GPT2Block: 3-2                         [1, 10, 768]              7,087,872\n",
              "│    │    └─GPT2Block: 3-3                         [1, 10, 768]              7,087,872\n",
              "│    │    └─GPT2Block: 3-4                         [1, 10, 768]              7,087,872\n",
              "│    │    └─GPT2Block: 3-5                         [1, 10, 768]              7,087,872\n",
              "│    │    └─GPT2Block: 3-6                         [1, 10, 768]              7,087,872\n",
              "│    │    └─GPT2Block: 3-7                         [1, 10, 768]              7,087,872\n",
              "│    │    └─GPT2Block: 3-8                         [1, 10, 768]              7,087,872\n",
              "│    │    └─GPT2Block: 3-9                         [1, 10, 768]              7,087,872\n",
              "│    │    └─GPT2Block: 3-10                        [1, 10, 768]              7,087,872\n",
              "│    │    └─GPT2Block: 3-11                        [1, 10, 768]              7,087,872\n",
              "│    │    └─GPT2Block: 3-12                        [1, 10, 768]              7,087,872\n",
              "│    └─LayerNorm: 2-5                              [1, 10, 768]              1,536\n",
              "├─Linear: 1-2                                      [1, 10, 50257]            38,597,376\n",
              "====================================================================================================\n",
              "Total params: 163,037,184\n",
              "Trainable params: 163,037,184\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 163.34\n",
              "====================================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 12.31\n",
              "Params size (MB): 652.15\n",
              "Estimated Total Size (MB): 664.46\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check model size"
      ],
      "metadata": {
        "id": "cHAvrfSQq0Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_count = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Parameters: {param_count:,}\")\n",
        "print(f\"Memory usage: ~{param_count * 4 / 1e9:.2f} GB (FP32)\")\n",
        "\n",
        "\n",
        "# Define bit-widths for different precisions\n",
        "# precisions = {\n",
        "#     \"FP32\": 32,\n",
        "#     \"FP16\": 16,\n",
        "#     \"INT8\": 8,\n",
        "#     \"INT4\": 4,\n",
        "# }\n",
        "\n",
        "# Print memory usage for each precision\n",
        "# print(\"Approximate Memory Usage:\")\n",
        "# for precision, bits in precisions.items():\n",
        "#     bytes_per_param = bits / 8\n",
        "#     memory_gb = param_count * bytes_per_param / 1e9\n",
        "#     print(f\"├── {precision}: ~{memory_gb:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUq-N4tGqhfR",
        "outputId": "1d7d1f4e-fd59-4fde-8c19-e3d0f2acf479"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: 124,439,808\n",
            "Memory usage: ~0.50 GB (FP32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference timing with the trained GPT-2 model"
      ],
      "metadata": {
        "id": "MQTzubgTq7c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "miEe5loJrFad"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model:` preloaded HuggingFace causal language model (e.g., GPT-2)\n",
        "`tokenizer:` convert text into model inputs\n",
        "`prompt:` text input to feed the model\n",
        "`num_runs:` times to measure the inference (default: 10 times for averaging)"
      ],
      "metadata": {
        "id": "rjRerY6krcOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_time(model, tokenizer, prompt, num_runs=10):\n",
        "    model.eval()\n",
        "    times = [] #init to store individual inference time\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Warmup inference without recording time to help eliminating the one-time memory allocation overhead. torch.no_grad for memory efficient inference, max token len set to 50\n",
        "    with torch.no_grad():\n",
        "        _ = model.generate(**inputs, max_length=50)\n",
        "\n",
        "    # record the inference times\n",
        "    for _ in range(num_runs):\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_length=50)\n",
        "        end = time.time()\n",
        "        times.append(end - start)\n",
        "\n",
        "    return sum(times) / len(times)\n",
        "\n",
        "avg_time = measure_inference_time(model, tokenizer, \"Hello, I am Jahid Hasan, AI researcher and entreprenuer.\")\n",
        "print(f\"Average inference time: {avg_time:.3f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDGGUSXDrK83",
        "outputId": "c46ae740-c3d8-43ff-cabc-6a7dd24a9e93"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average inference time: 2.050 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the precision formats, experiment with quantization"
      ],
      "metadata": {
        "id": "oeyOgqEZuJYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "J5dR63WBuP-8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_with_precision(model_name, precision=\"fp32\"):\n",
        "    if precision == \"fp16\":\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "    elif precision == \"int8\":\n",
        "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "    elif precision == \"int4\":\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "    else:  # fp32\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "8j6maduxuUtE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precisions = [\"fp32\", \"fp16\", \"int8\", \"int4\"]\n",
        "results = {}"
      ],
      "metadata": {
        "id": "IqqPnhCUuZEk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for precision in precisions:\n",
        "    print(f\"\\nTesting {precision.upper()}...\")\n",
        "    try:\n",
        "        model = load_model_with_precision(\"microsoft/DialoGPT-small\", precision)\n",
        "\n",
        "        # Measure memory and speed\n",
        "        memory_mb = torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0\n",
        "        inference_time = measure_inference_time(model, tokenizer, \"Hello\")\n",
        "\n",
        "        results[precision] = {\n",
        "            'memory_mb': memory_mb,\n",
        "            'inference_time': inference_time\n",
        "        }\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {precision}: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859,
          "referenced_widgets": [
            "8012267ae7dc43e18dcc248441077784",
            "895cabbe8d3e445aaf21232afa306fb5",
            "5891a3e1dba0459aad1ba1c529d63601",
            "edee89e07bd648798b98a8053bd7ca05",
            "307c3f095d2f48ce8af17353410ed134",
            "b902fe333f0b4d57aa40d45d8a3beaf9",
            "0dfffcfa5b69473fb958d3f78e968fde",
            "0dfbc024a76a430c94b7b64ca35fd922",
            "98576cea912d4ff2a55f329bbb090746",
            "271bb0cd79314011b7fc79869495a7f7",
            "166e4b84f6854d889146474f910dbe86",
            "d33ac0aed9f14b83bde22a89a0a77975",
            "0e21a2f5dd5745db9c1887bdbb1b4ac3",
            "772d80382ea645159e7799cf48bdaffa",
            "030fc99e8b6f462491792db5033d7e74",
            "4f3d06116d40496eb9f9e26d5459c096",
            "211bbeaae23c4e399f06749b51ad7190",
            "dd6e459c45db4f5ea18600b808bb8333",
            "f0b6f3b97a8a4dfe8e4036cc1064af28",
            "2ac873fe7be44f84a3fd0349a93bdd9e",
            "4baf47ca23c040e78f0a935b6e9b2269",
            "5d29d80261d24f919faa666d51b539c5",
            "c062447b0b6245c4ad73e736310ffd5b",
            "ccb81728fd0e43a08dda2118b610bbe0",
            "ba0be84f70fa4f3ea87ecf919c23d365",
            "8cc08cca64734142991ab0f874cae0b0",
            "4da0c81e2911432ab3a9ed3b38cd3573",
            "0f69276926354b0a8900716dfcb5cce4",
            "859ca557dcb7432bbda46b86ce2f7fff",
            "07f835d622644e6895c216c76d348b37",
            "5530599f7d2b49dfb2d149e2378726aa",
            "14f004052bc54a6c82c9c045decf18cf",
            "dd26dea94be4479d9ca5404b1931fa8d"
          ]
        },
        "id": "AJzcoiRkub3j",
        "outputId": "d60d2f07-08b6-4b0f-e132-78579fbe1db3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing FP32...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8012267ae7dc43e18dcc248441077784"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d33ac0aed9f14b83bde22a89a0a77975"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c062447b0b6245c4ad73e736310ffd5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing FP16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing INT8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing INT4...\n",
            "Error with int4: quant_type must be nf4 on CPU, got fp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(results).T\n",
        "print(\"\\nQuantization Comparison:\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_kTWZw9upEz",
        "outputId": "f00364be-675b-4d2d-a839-5826742f9210"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Quantization Comparison:\n",
            "      memory_mb  inference_time\n",
            "fp32        0.0        0.219912\n",
            "fp16        0.0        1.879117\n",
            "int8        0.0        8.840728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-Training Quantization (PTQ)"
      ],
      "metadata": {
        "id": "j0TXIRgPvbVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using GPTQ for better 4-bit quantization\n",
        "# !pip install auto-gptq"
      ],
      "metadata": {
        "id": "uZXzy2jxva_M"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVpR-6nsvocp",
        "outputId": "93b785e9-3ada-439b-8cb9-be1ed83557a9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Static Batch inference"
      ],
      "metadata": {
        "id": "-FzVCQpBvuvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def static_batch_inference(model, tokenizer, prompts, batch_size=4):\n",
        "    \"\"\"Process prompts in fixed-size batches\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Pad tokenizer for batching\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        batch = prompts[i:i+batch_size]\n",
        "\n",
        "        # Tokenize batch\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode results\n",
        "        batch_results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        results.extend(batch_results)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Q_gnjlrivwjS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"The future of AI is\",\n",
        "    \"Machine learning helps us\",\n",
        "    \"Deep learning models can\",\n",
        "    \"Neural networks are\",\n",
        "    \"Artificial intelligence will\"\n",
        "]\n",
        "\n",
        "batch_results = static_batch_inference(model, tokenizer, prompts, batch_size=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xZeJIuPv0OL",
        "outputId": "7675b9a6-9593-4087-c31a-8419736bd597"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic batch inference"
      ],
      "metadata": {
        "id": "ZgFQ9xq3wEEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import random\n",
        "from queue import Queue\n",
        "from threading import Thread\n",
        "import time"
      ],
      "metadata": {
        "id": "L1cxDTKCwGZR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicBatcher:\n",
        "    def __init__(self, model, tokenizer, max_batch_size=4, max_wait_time=0.1):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_batch_size = max_batch_size\n",
        "        self.max_wait_time = max_wait_time\n",
        "        self.request_queue = Queue()\n",
        "        self.running = True\n",
        "\n",
        "        # Start background processing\n",
        "        self.processor_thread = Thread(target=self._process_batches)\n",
        "        self.processor_thread.start()\n",
        "    def _process_batches(self):\n",
        "        while self.running:\n",
        "            batch = []\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Collect requests for batch\n",
        "            while (len(batch) < self.max_batch_size and\n",
        "                   time.time() - start_time < self.max_wait_time):\n",
        "\n",
        "                if not self.request_queue.empty():\n",
        "                    request = self.request_queue.get()\n",
        "                    batch.append(request)\n",
        "                else:\n",
        "                    time.sleep(0.001)  # Small sleep to prevent busy waiting\n",
        "\n",
        "            if batch:\n",
        "                self._process_batch(batch)\n",
        "\n",
        "    def _process_batch(self, batch):\n",
        "        prompts = [req['prompt'] for req in batch]\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        # Generate\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(**inputs, max_new_tokens=20)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        # Return results\n",
        "        results = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        for i, req in enumerate(batch):\n",
        "            req['result'] = results[i]\n",
        "            req['processing_time'] = processing_time\n",
        "            req['batch_size'] = len(batch)\n",
        "\n",
        "    def inference(self, prompt):\n",
        "        request = {'prompt': prompt, 'result': None, 'processing_time': None}\n",
        "        self.request_queue.put(request)\n",
        "\n",
        "        # Wait for result\n",
        "        while request['result'] is None:\n",
        "            time.sleep(0.001)\n",
        "\n",
        "        return request\n",
        "\n",
        "    def stop(self):\n",
        "        self.running = False\n",
        "        self.processor_thread.join()"
      ],
      "metadata": {
        "id": "eonWahxLwJ35"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test dynamic batching\n",
        "batcher = DynamicBatcher(model, tokenizer)"
      ],
      "metadata": {
        "id": "nU8S0IfAwSAq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate concurrent requests\n",
        "results = []\n",
        "for i in range(10):\n",
        "    result = batcher.inference(f\"Request {i}: The answer is\")\n",
        "    results.append(result)\n",
        "    time.sleep(random.uniform(0.01, 0.05))  # Random arrival times\n",
        "\n",
        "batcher.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7TMyADowUtQ",
        "outputId": "f3950b8e-a31c-4da9-93db-77ef9bc259c3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "J2smcv7ewdEg"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [r['batch_size'] for r in results]\n",
        "print(f\"Average batch size: {sum(batch_sizes) / len(batch_sizes):.2f}\")\n",
        "print(f\"Batch size distribution: {dict(zip(*np.unique(batch_sizes, return_counts=True)))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCnvu1QowbFi",
        "outputId": "168182c6-4a74-46f5-840a-c45ee39afa03"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average batch size: 1.00\n",
            "Batch size distribution: {np.int64(1): np.int64(10)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key-Value Cache Management for continuous batching and quantization for advance optimization"
      ],
      "metadata": {
        "id": "CdSrubt7wkkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KVCacheManager:\n",
        "    \"\"\"Manages KV cache for efficient generation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cache = {}\n",
        "\n",
        "    def get_cache_size(self, past_key_values):\n",
        "        \"\"\"Calculate memory usage of KV cache\"\"\"\n",
        "        if past_key_values is None:\n",
        "            return 0\n",
        "\n",
        "        total_size = 0\n",
        "        for layer_cache in past_key_values:\n",
        "            for tensor in layer_cache:\n",
        "                total_size += tensor.numel() * tensor.element_size()\n",
        "\n",
        "        return total_size / (1024**2)  # MB\n",
        "\n",
        "    def efficient_generate(self, model, tokenizer, prompt, max_new_tokens=50):\n",
        "        \"\"\"Generate with KV cache tracking\"\"\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        generated_tokens = []\n",
        "        past_key_values = None\n",
        "        current_input = inputs['input_ids']\n",
        "\n",
        "        for step in range(max_new_tokens):\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=current_input,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True\n",
        "            )\n",
        "\n",
        "            # Get next token\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "            generated_tokens.append(next_token.item())\n",
        "\n",
        "            # Update for next iteration\n",
        "            past_key_values = outputs.past_key_values\n",
        "            current_input = next_token\n",
        "\n",
        "            # Track cache size\n",
        "            cache_size = self.get_cache_size(past_key_values)\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                print(f\"Step {step}: Cache size = {cache_size:.2f} MB\")\n",
        "\n",
        "        # Decode result\n",
        "        full_sequence = torch.cat([inputs['input_ids'], torch.tensor([generated_tokens])], dim=1)\n",
        "        return tokenizer.decode(full_sequence[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "eUMGZxG0wff6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efficiency test KV"
      ],
      "metadata": {
        "id": "olCmd5bXww4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache_manager = KVCacheManager()\n",
        "result = cache_manager.efficient_generate(model, tokenizer, \"The benefits of caching are\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ5lPRrdwv6K",
        "outputId": "b83d7ff0-9a65-445a-8cd3-d786bd2519d7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. You should pass an instance of `Cache` instead, e.g. `past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Cache size = 0.35 MB\n",
            "Step 10: Cache size = 1.05 MB\n",
            "Step 20: Cache size = 1.76 MB\n",
            "Step 30: Cache size = 2.46 MB\n",
            "Step 40: Cache size = 3.16 MB\n",
            "The benefits of caching are that you can easily see the data in the cache and you can easily see the changes in the cache.\n",
            "\n",
            "The caching is done by using a cache object that is a collection of objects that are stored in the cache. The objects are stored in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastAPI Experiment"
      ],
      "metadata": {
        "id": "1WJzgt1FxRfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fastapi uvicorn nest-asyncio pyngrok transformers accelerate torch"
      ],
      "metadata": {
        "id": "MqJln1vaxTR4"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken API_KEY_HERE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqzQcyXnxm_V",
        "outputId": "243ae842-4741-44fc-8a7b-5de42a63930c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set up FastAPI server\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvFCo6bVxYRp",
        "outputId": "a2e60ba4-ccdb-4e2f-fdf2-270a5de932be"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://e211-34-86-98-232.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import time\n",
        "\n",
        "# Patch asyncio event loop for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Inference input model\n",
        "class InferenceRequest(BaseModel):\n",
        "    prompt: str\n",
        "    max_tokens: int = 50\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 0.9\n",
        "\n",
        "# Inference output model\n",
        "class InferenceResponse(BaseModel):\n",
        "    generated_text: str\n",
        "    tokens_generated: int\n",
        "    inference_time: float\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Load model and tokenizer (once)\n",
        "model_name = \"microsoft/DialoGPT-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.eval()\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Inference server is running!\"}\n",
        "\n",
        "@app.post(\"/generate\", response_model=InferenceResponse)\n",
        "def generate(request: InferenceRequest):\n",
        "    try:\n",
        "        start = time.time()\n",
        "        inputs = tokenizer(request.prompt, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=request.max_tokens,\n",
        "                temperature=request.temperature,\n",
        "                top_p=request.top_p,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        end = time.time()\n",
        "        return InferenceResponse(\n",
        "            generated_text=generated_text,\n",
        "            tokens_generated=len(output[0]) - len(inputs[\"input_ids\"][0]),\n",
        "            inference_time=end - start\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n"
      ],
      "metadata": {
        "id": "YxGbNI_31DFW"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "class BatchInferenceRequest(BaseModel):\n",
        "    prompts: List[str]\n",
        "    max_tokens: int = 50\n",
        "    temperature: float = 0.7\n",
        "\n",
        "@app.post(\"/batch_generate\")\n",
        "def batch_generate(request: BatchInferenceRequest):\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            request.prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=request.max_tokens,\n",
        "                temperature=request.temperature,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        results = []\n",
        "        for i, output in enumerate(outputs):\n",
        "            text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "            results.append({\n",
        "                \"prompt_index\": i,\n",
        "                \"generated_text\": text\n",
        "            })\n",
        "\n",
        "        inference_time = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"results\": results,\n",
        "            \"batch_size\": len(request.prompts),\n",
        "            \"total_inference_time\": inference_time,\n",
        "            \"average_time_per_prompt\": inference_time / len(request.prompts)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n"
      ],
      "metadata": {
        "id": "af9qwpT01lzV"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNZKC2Qs1Nz2",
        "outputId": "c3b458da-714c-44b4-d5b0-d890ece4061d"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1330]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     34.69.214.222:0 - \"POST /generate HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     34.69.214.222:0 - \"POST /batch_generate HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [1330]\n"
          ]
        }
      ]
    }
  ]
}